{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sequence_2_Sequence.ipynb","provenance":[{"file_id":"1a3h3DefIKqxemcX5DJ8EOr7XprXKkxiS","timestamp":1649260024570}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kGnrBg46TDtR"},"source":["## Seq2Seq Model - Neural Machine Translation\n","- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n","- https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n","- https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/"]},{"cell_type":"markdown","metadata":{"id":"MGNubSuHTDtV"},"source":["<img width=\"60%\" src=\"https://machinetalk.org/wp-content/uploads/2019/03/attention.gif\" class=\"img-responsive wp-post-image\" alt=\"\" data-pagespeed-url-hash=\"1740859731\" onload=\"pagespeed.CriticalImages.checkImageForCriticality(this);\">"]},{"cell_type":"markdown","metadata":{"id":"fAY9Y6n4TDtV"},"source":["### What is <b>sequence-to-sequence learning<b>?"]},{"cell_type":"markdown","metadata":{"id":"S2BLsnmlTDtW"},"source":["Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).\n","\n","\"the cat sat on the mat\" -> <b>[Seq2Seq model] </b>-> \"le chat etait assis sur le tapis\"\n","\n","This can be used for machine translation or for free-from question answering (generating a natural language answer given a natural language question) -- in general, it is applicable any time you need to generate text."]},{"cell_type":"markdown","metadata":{"id":"fKyBJ-dxTDtW"},"source":["### The general case: canonical sequence-to-sequence"]},{"cell_type":"markdown","metadata":{"id":"ESisiihHTDtX"},"source":["In the general case, input sequences and output sequences have different lengths (e.g. machine translation) and the entire input sequence is required in order to start predicting the target. This requires a more advanced setup, which is what people commonly refer to when mentioning \"sequence to sequence models\" with no further context. Here's how it works:"]},{"cell_type":"markdown","metadata":{"id":"bUxYgiCBTDtX"},"source":["<img alt=\"Seq2seq inference\" src=\"https://miro.medium.com/max/1400/1*0aHodc667UfSyZj-UY8OQw.png\" width=\"60%\">\n"]},{"cell_type":"markdown","metadata":{"id":"Cmp6CVjxTDtX"},"source":["- A RNN layer (or stack thereof) acts as <b>\"encoder\"</b>: it processes the input sequence and returns its own internal state. The encoder, which is on the left-hand side, requires only sequences from source language as inputs. Note that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bMTFFF09TDtY"},"source":["- Another RNN layer (or stack thereof) acts as <b>\"decoder\"</b>: it is trained to predict the next word of the target sequence, given previous words of the target sequence. \n","    - Specifically, it is trained to `turn the target sequences into the same sequences but offset by one timestep in the future`, a training process called `teacher forcing` in this context. \n","    - Importantly, `the encoder uses as initial state the state vectors from the encoder`, which is how the decoder obtains information about what it is supposed to generate.\n","    - Effectively, the decoder learns to generate target at $t+1$ given target at $t$, conditioned on the input sequence.\n","    - The same process can also be used to train a Seq2Seq network `without teacher forcing`, i.e. by reinjecting the decoder's predictions into the decoder."]},{"cell_type":"markdown","metadata":{"id":"Jm9fbUP_TDtY"},"source":["## Neural Translation Machine"]},{"cell_type":"markdown","metadata":{"id":"HtJLiaGtTDtZ"},"source":["Let's illustrate these ideas with actual code.\n","\n","For our example implementation, we will use a dataset of pairs of English sentences and their French translation, which you can download from <a href=\"http://www.manythings.org/anki/\">manythings.org/anki</a>. The file to download is called `fra-eng.zip` (English/French). We will implement a word-level model sequence-to-sequence model, processing the input word-by-word and generating the output word-by-word. \n","\n","Here's a summary of our process:\n","\n","- 1) Turn the sentences into 3 Numpy arrays, `encoder_input_data`, `decoder_input_data`, `decoder_target_data`:\n","    - `encoder_input_data` is a 3D array of shape (`num_pairs`, `max_english_sentence_length`, `num_english_characters`) containing a one-hot vectorization of the English sentences.\n","    - `decoder_input_data` is a 3D array of shape (`num_pairs`, `max_french_sentence_length`, `num_french_characters`) containg a one-hot vectorization of the French sentences.\n","    - `decoder_target_data` is the same as decoder_input_data but offset by one timestep. `decoder_target_data[:, t, :]` will be the same as `decoder_input_data[:, t + 1, :]`.\n","- 2) Train a basic LSTM-based Seq2Seq model to predict `decoder_target_data` given `encoder_input_data` and `decoder_input_data`. Our model uses `teacher forcing`.\n","- 3) Decode some sentences to check if the model is working (i.e. turn samples from `encoder_input_data` into corresponding samples from `decoder_target_data`).\n","\n","Because the `training process` and `inference process` (decoding sentences) are quite different, `we use different models for both, albeit they all leverage the same inner layers`.\n","\n","Note that the encoder and decoder are connected by RNN states:\n","\n","- `encoder states`: This is used to store the states of the encoder.\n","- `inital_state of decoder`: we pass the encoder states to the decoder as initial states.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xkP-yOF6UFUR","executionInfo":{"status":"ok","timestamp":1649262763222,"user_tz":240,"elapsed":11559,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"58091520-de8f-4792-bc00-bec7d175b7bd"},"source":["!pip install torchinfo"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.6.5-py3-none-any.whl (21 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.6.5\n"]}]},{"cell_type":"code","metadata":{"id":"j0G2VAsTTDta","executionInfo":{"status":"ok","timestamp":1649262768486,"user_tz":240,"elapsed":5268,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","import numpy as np\n","import unicodedata\n","import re\n","from tensorflow import keras\n","import pandas as pd\n","from sklearn.utils import shuffle"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"898_SKUGTDtb"},"source":["### (1) Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"vlxqKSoMTDtb"},"source":["1) Turn the sentences into 3 Numpy arrays, `data_en`, `data_fr_in`, `data_fr_out`:\n","- `data_en`: is a 2D array of shape (`num_samples`, `max_en_words_per_sentence`) containing a tokenized sentences after preprocessing.\n","- `data_fr_in`: is a 2D array of shape (`num_samples`, `max_fr_words_per_sentence`) containing a tokenized sentences after preprocessing.\n","- `data_fr_out`: the same as decoder_input_data but offset by one timestep, i.e.  `data_fr_in [:, t]` will be the same as `data_fr_out [:, t+1]`.  "]},{"cell_type":"markdown","metadata":{"id":"22OH8g2dTDtc"},"source":["Note that, this is a demo version of the NTM. We will train the model using a small dataset. To make the model work realistically, you need to train the model with a larget collection of training samples\n","\n","\n","We'll use Keras for simple data preprocessing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jwQ1aLeaWCZ7","executionInfo":{"status":"ok","timestamp":1649262995433,"user_tz":240,"elapsed":720,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"eb826358-6222-4203-da66-cb85ade20369"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Point to the training data file\n","path_to_glove_file = \"drive/MyDrive/BIA667_Lab/fra.txt\""],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"id":"pxGrDvhrTDtc","executionInfo":{"status":"ok","timestamp":1649262997858,"user_tz":240,"elapsed":1124,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"979d45e8-2385-48bc-9a48-a4936a1a5ce2"},"source":["# Read data\n","#text = pd.read_csv('fra.txt', sep=\"\\t\", header=None, usecols=[0,1])\n","text = pd.read_csv(path_to_glove_file, sep=\"\\t\", header=None, usecols=[0,1])\n","text.columns =['en','fr']\n","text.head()\n","len(text)\n","\n","# Take a small set to save training time\n","text = shuffle(text)\n","raw_data=text.iloc[0:10000]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     en        fr\n","0   Go.      Va !\n","1   Hi.   Salut !\n","2   Hi.    Salut.\n","3  Run!   Cours !\n","4  Run!  Courez !"],"text/html":["\n","  <div id=\"df-e992b793-790a-4e2b-9dbc-6f5fe9394383\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>en</th>\n","      <th>fr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Hi.</td>\n","      <td>Salut.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e992b793-790a-4e2b-9dbc-6f5fe9394383')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e992b793-790a-4e2b-9dbc-6f5fe9394383 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e992b793-790a-4e2b-9dbc-6f5fe9394383');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12},{"output_type":"execute_result","data":{"text/plain":["175623"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"APHjwbk3TDte","executionInfo":{"status":"ok","timestamp":1649263019048,"user_tz":240,"elapsed":87,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["# clean up text\n","\n","def unicode_to_ascii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","\n","def normalize_string(s):\n","    s = unicode_to_ascii(s)\n","    s = re.sub(r'([!.?])', r' \\1', s)\n","    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n","    s = re.sub(r'\\s+', r' ', s)\n","    return s"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mn_5sRfKTDte","executionInfo":{"status":"ok","timestamp":1649263027741,"user_tz":240,"elapsed":743,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["# clean up text\n","raw_data_en = [normalize_string(data) for data in raw_data[\"en\"]]\n","\n","# add special token <start>/<end> to indicate the beginning and end of a sentence\n","raw_data_fr_in = ['<start> ' + normalize_string(data) for data in raw_data[\"fr\"]]\n","raw_data_fr_out = [normalize_string(data) + ' <end>' for data in raw_data[\"fr\"]]"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0_K5J-mTDte","executionInfo":{"status":"ok","timestamp":1649263033328,"user_tz":240,"elapsed":210,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"e86d7fae-1595-478c-885d-cd81f412963d"},"source":["# Tokenize each sentence and index each word\n","max_en_words = 5000\n","max_en_len = 10\n","en_tokenizer = keras.preprocessing.text.Tokenizer(filters='', \\\n","                                                  num_words=max_en_words )\n","en_tokenizer.fit_on_texts(raw_data_en)\n","print(\"Total number of English words: \", len(en_tokenizer.word_index))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of English words:  4616\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"peO39yU1TDtf","executionInfo":{"status":"ok","timestamp":1649263035765,"user_tz":240,"elapsed":238,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"9dda4e2a-9fab-4b68-8baa-6530ef0104c7"},"source":["data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n","data_en = keras.preprocessing.sequence.pad_sequences(data_en,\\\n","                                                     maxlen=max_en_len, \\\n","                                                     padding='post')\n","# print a sample sentence after preprocessing\n","print(data_en[:3])"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  24  418    4   31  186   52   66  104  516    1]\n"," [   3   63    8  331    4  549   17    1    0    0]\n"," [   2   28    8 1380    1    0    0    0    0    0]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sc0cEYKJTDtf","executionInfo":{"status":"ok","timestamp":1649263040714,"user_tz":240,"elapsed":1120,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"fee47dd2-38a9-44e1-d36a-dbfb277860ed"},"source":["# Process French sentences in the same way\n","\n","max_fr_words = 5000\n","max_fr_len = 10\n","fr_tokenizer = keras.preprocessing.text.Tokenizer(filters='', num_words = max_fr_words)\n","\n","# ATTENTION: always finish with fit_on_texts before moving on\n","fr_tokenizer.fit_on_texts(raw_data_fr_in)\n","fr_tokenizer.fit_on_texts(raw_data_fr_out)\n","print(\"Total number of French words: \", len(fr_tokenizer.word_index))\n","\n","data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)\n","data_fr_in = keras.preprocessing.sequence.pad_sequences(data_fr_in,\\\n","                                                        maxlen=max_fr_len, \\\n","                                                        padding='post')\n","\n","data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)\n","data_fr_out = keras.preprocessing.sequence.pad_sequences(data_fr_out,\\\n","                                                            maxlen=max_fr_len, \\\n","                                                            padding='post')\n","# print a sample sentence after preprocessing\n","data_fr_in[:3]"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of French words:  6444\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[   5,    5,   35,   63,  115,   50, 1134,   21,  119,    1],\n","       [   2,   10,  884,   26,  320,   32, 1001,  126,    1,    0],\n","       [   2,    4,   29, 2029,    1,    0,    0,    0,    0,    0]],\n","      dtype=int32)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPCKtESXTDtg","executionInfo":{"status":"ok","timestamp":1649263045088,"user_tz":240,"elapsed":84,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"9ea7c08d-aee7-4fb4-f319-05dd9d2aecc4"},"source":["# Create the reversal mapping between indexes and words\n","reverse_fr_word_index ={fr_tokenizer.word_index[w] : w \\\n","                        for w in  fr_tokenizer.word_index}\n","print(\"index of symbol <start> :\", fr_tokenizer.word_index[\"<start>\"])\n","print(\"index of symbol <end> :\", fr_tokenizer.word_index[\"<end>\"])"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["index of symbol <start> : 2\n","index of symbol <end> : 3\n"]}]},{"cell_type":"markdown","metadata":{"id":"Upb8hh2UTDtg"},"source":["### (2) Define \"Teacher Forcing\" Model for Training Process"]},{"cell_type":"markdown","metadata":{"id":"EtQPaUm2TDth"},"source":["2) Train a basic LSTM-based Seq2Seq model to predict `decoder_outputs` given `encoder_inputs` and `decoder_inputs`. Our model uses `teacher forcing`.\n","\n","\n","And here is how the data’s shape changes at each layer. Often keeping track of the data’s shape is extremely helpful not to make silly mistakes, just like stacking up Lego pieces.\n","\n","Here we start with a simple encoder: only one layer LSTM, unidirectional.\n","\n","Task for you:\n","`Can you modify the model to allow multiple layers and bidirectional?`"]},{"cell_type":"code","metadata":{"id":"SEprU1ODTDth","executionInfo":{"status":"ok","timestamp":1649263075458,"user_tz":240,"elapsed":6436,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","import torch.optim as optim"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"i2j_DkOHWk0z","executionInfo":{"status":"ok","timestamp":1649263075459,"user_tz":240,"elapsed":4,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"NyDE9y-3TDti","executionInfo":{"status":"ok","timestamp":1649263075459,"user_tz":240,"elapsed":3,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["max_en_words = 5000\n","max_en_len = 10\n","max_fr_len = 10\n","max_fr_words = 5000\n","latent_dim = 100   #i.e. rnn_size\n","batch_size = 32"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3huf1cNTDti","executionInfo":{"status":"ok","timestamp":1649263301699,"user_tz":240,"elapsed":178,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["# vocab_size: the total number of words in vocabulary\n","# embedding_size: word embedding dimension\n","# latent_dim: RNN hidden state dimension\n","\n","class EncoderLSTM(nn.Module):\n","    \n","  def __init__(self, vocab_size, embedding_size, hidden_size):\n","    \n","    super(EncoderLSTM, self).__init__()\n","    \n","    self.vocab_size = vocab_size\n","\n","    self.embedding_size = embedding_size\n","    \n","    self.hidden_size = hidden_size\n","\n","    self.embedding = nn.Embedding(self.vocab_size, self.embedding_size,padding_idx=0)\n","    \n","    # The input to LSTM should be [batch_size, seq_length, embedding_size]\n","    self.LSTM = nn.LSTM(input_size = self.embedding_size, \\\n","                        hidden_size = self.hidden_size,\n","                        batch_first = True)\n","\n","  \n","  def forward(self, x):\n","    \n","    # the shape of x is [batch_size, seq_length]\n","    x = self.embedding(x)  \n","    # after embeding, the shape of x is: [batch_size, seq_length, embedding_size]\n","    \n","    # We don't care about output. We only need states\n","    outputs, (hidden_state, cell_state) = self.LSTM(x)\n","    # hidden_state shape: [1, batch_size, hidden_size]\n","    # cell_state shape: [1, batch_size, hidden_size]\n","    \n","    #print(hidden_state.shape)\n","    #print(cell_state.shape)\n","    \n","    return outputs, hidden_state, cell_state"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtwjkm3UTDtj","executionInfo":{"status":"ok","timestamp":1649263305615,"user_tz":240,"elapsed":478,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"dc99082d-0a3a-4c31-a4e4-771ad6d647af"},"source":["encoder = EncoderLSTM(vocab_size=max_en_words,\n","                      embedding_size=latent_dim,\n","                      hidden_size=latent_dim)\n","\n","from torchinfo import summary \n","summary(encoder,input_size=(batch_size,max_en_len),\n","       dtypes=[torch.long])"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","EncoderLSTM                              --                        --\n","├─Embedding: 1-1                         [32, 10, 100]             500,000\n","├─LSTM: 1-2                              [32, 10, 100]             80,800\n","==========================================================================================\n","Total params: 580,800\n","Trainable params: 580,800\n","Non-trainable params: 0\n","Total mult-adds (M): 41.86\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.51\n","Params size (MB): 2.32\n","Estimated Total Size (MB): 2.84\n","=========================================================================================="]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"MNMtvBiDTDtj","executionInfo":{"status":"ok","timestamp":1649263367437,"user_tz":240,"elapsed":107,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["# vocab_size: the total number of words in vocabulary\n","# embedding_size: word embedding dimension\n","# latent_dim: RNN hidden state dimension\n","\n","class DecoderLSTM(nn.Module):\n","    \n","  def __init__(self, vocab_size, embedding_size, hidden_size):\n","    \n","    super(DecoderLSTM, self).__init__()\n","    \n","    self.vocab_size = vocab_size\n","\n","    self.embedding_size = embedding_size\n","    \n","    self.hidden_size = hidden_size\n","\n","    self.embedding = nn.Embedding(self.vocab_size, self.embedding_size,padding_idx=0)\n","    \n","    # The input to LSTM should be [batch_size, seq_length, embedding_size]\n","    self.LSTM = nn.LSTM(input_size = self.embedding_size, \\\n","                        hidden_size = self.hidden_size,\n","                        batch_first = True)\n","\n","    self.dense = nn.Linear(in_features = self.hidden_size, \n","                        out_features = self.vocab_size)\n","\n","\n","  def forward(self, x, hidden_state, cell_state):\n","    \n","    \n","    x = self.embedding(x)\n","\n","    # LSTM will be initialized with encoder states\n","    outputs, (h, c) = self.LSTM(x, (hidden_state, cell_state))\n","    # outputs shape is [batch, seq_length, hidden_state]\n","    \n","    predictions = self.dense(outputs)\n","    # prediction shape is [batch, seq_length, vocab_size]\n","\n","    return predictions, h, c"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R10iQbTfTDtk","executionInfo":{"status":"ok","timestamp":1649263370797,"user_tz":240,"elapsed":98,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"a62fc1f7-e679-44f1-c9c5-2232e7bba861"},"source":["decoder = DecoderLSTM(vocab_size=max_fr_words,\\\n","                      embedding_size=latent_dim,\\\n","                      hidden_size=latent_dim)\n","\n","summary(decoder,input_size=[(batch_size,max_fr_len),\\\n","                            (1,batch_size,latent_dim),\\\n","                            (1,batch_size,latent_dim)],\\\n","        dtypes=[torch.long,torch.float,torch.float])"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","DecoderLSTM                              --                        --\n","├─Embedding: 1-1                         [32, 10, 100]             500,000\n","├─LSTM: 1-2                              [32, 10, 100]             80,800\n","├─Linear: 1-3                            [32, 10, 5000]            505,000\n","==========================================================================================\n","Total params: 1,085,800\n","Trainable params: 1,085,800\n","Non-trainable params: 0\n","Total mult-adds (M): 58.02\n","==========================================================================================\n","Input size (MB): 0.03\n","Forward/backward pass size (MB): 13.31\n","Params size (MB): 4.34\n","Estimated Total Size (MB): 17.68\n","=========================================================================================="]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"zZvPsEMfTDtm"},"source":["#### Connect Encoder and Decoder to Create Seq2Seq Model ####"]},{"cell_type":"code","metadata":{"id":"DdRFQ_KSTDtn","executionInfo":{"status":"ok","timestamp":1649263392169,"user_tz":240,"elapsed":106,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["class Seq2Seq(nn.Module):\n","    \n","  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n","    \n","    super(Seq2Seq, self).__init__()\n","    \n","    self.Encoder = Encoder_LSTM\n","    self.Decoder = Decoder_LSTM\n","\n","  def forward(self, encoder_input, decoder_input):\n","    \n","    _, hidden_state, cell_state = self.Encoder(encoder_input)\n","    \n","    predictions, _, _ = self.Decoder(decoder_input, hidden_state, cell_state)\n","        \n","    return predictions\n"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"402ZnbfOTDto","executionInfo":{"status":"ok","timestamp":1649263406045,"user_tz":240,"elapsed":112,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["model = Seq2Seq(encoder, decoder)"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lPEF4p9STDtp"},"source":["#### Create Dataset and Training Function ####"]},{"cell_type":"markdown","metadata":{"id":"d8Iu7R0OTDtp"},"source":["Now you can compile and fit the model as usual. Note the matching between tensors and variables:\n","- `encoder_inputs` <-> `data_en`\n","- `decoder_inputs` <-> `data_fr_in`\n","- `decoder_outputs` <-> `data_fr_out`"]},{"cell_type":"code","metadata":{"id":"lxlSiFSETDtq","executionInfo":{"status":"ok","timestamp":1649263414891,"user_tz":240,"elapsed":97,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["class NTM_dataset(Dataset):\n","    \n","    def __init__(self,data_en, data_fr_in,data_fr_out):\n","        \n","        self.length = len(data_en)\n","        \n","        self.encoder_input = torch.IntTensor(data_en)\n","        self.decoder_input = torch.IntTensor(data_fr_in)\n","        \n","        # for CrossEntropyLoss, decoder_output must have Long data type\n","        self.decoder_output = torch.LongTensor(data_fr_out)\n","    \n","    def __getitem__(self, index):\n","        return self.encoder_input[index], \\\n","               self.decoder_input[index],\\\n","               self.decoder_output[index]\n","    \n","    def __len__(self):\n","        return self.length "],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4x0TxU7TDtq","executionInfo":{"status":"ok","timestamp":1649263424942,"user_tz":240,"elapsed":110,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["dataset = NTM_dataset(data_en, data_fr_in, data_fr_out)\n","\n","test_size = int(len(data_en) * 0.2)\n","train_size = len(data_en) - test_size\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, \\\n","                                                            [train_size, test_size])\n"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofAcEfmWTDtq","executionInfo":{"status":"ok","timestamp":1649263447215,"user_tz":240,"elapsed":92,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"9840b0ad-8ba1-4bdb-fd2a-d0fe418b24c8"},"source":["print(len(train_dataset))\n","len(test_dataset)"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["8000\n"]},{"output_type":"execute_result","data":{"text/plain":["2000"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"2nkhNF_4TDtr","executionInfo":{"status":"ok","timestamp":1649263487526,"user_tz":240,"elapsed":211,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["# Define a function to train the model \n","def train_model(model, train_dataset, test_dataset, device, lr=0.0005, epochs=20, batch_size=32):\n","    \n","    # construct dataloader\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","    # move model to device\n","    model = model.to(device)\n","\n","    # history\n","    history = {'train_loss': [],\n","               'train_acc': [],\n","               'test_loss': [],\n","               'test_acc': []}\n","    # setup loss function and optimizer\n","    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # training loop\n","    print('Training Start')\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0\n","        train_acc = 0\n","        test_loss = 0\n","        test_acc = 0\n","\n","        for encoder_input, decoder_input, decoder_output in train_loader:\n","            \n","            # move data to device\n","            encoder_input = encoder_input.to(device)\n","            decoder_input = decoder_input.to(device)\n","            decoder_output = decoder_output.to(device)\n","            \n","            # forward\n","            outputs = model(encoder_input, decoder_input)  # batch_size, max_fr_len (i.e. seq_len), fr_vocab_size\n","            \n","            _, pred = torch.max(outputs, dim = -1)\n","            \n","            #reshape output to batch_size * seq_len, fr_vocab_size since the loss looks for 2-dimensional input                \n","            cur_train_loss = criterion(outputs.view(-1 , max_fr_words), decoder_output.view(-1))\n","            \n","            # reshape pred & decoder ouput t to calculate acc of each predicted words\n","            cur_train_acc = (pred.view(-1) == decoder_output.view(-1)) \n","            cur_train_acc = cur_train_acc.sum().item()/len(cur_train_acc)\n","                \n","            # backward\n","            cur_train_loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            # loss and acc\n","            train_loss += cur_train_loss\n","            train_acc += cur_train_acc\n","\n","        # test start\n","        model.eval()\n","        with torch.no_grad():\n","            \n","            for encoder_input, decoder_input, decoder_output in test_loader:\n","            \n","                # move data to device\n","                encoder_input = encoder_input.to(device)\n","                decoder_input = decoder_input.to(device)\n","                decoder_output = decoder_output.to(device)\n","            \n","                # forward\n","                outputs = model(encoder_input, decoder_input)  # batch_size, max_fr_len (i.e. seq_len), fr_vocab_size\n","            \n","                _, pred = torch.max(outputs, dim = -1)\n","            \n","                #reshape output to batch_size, seq_len * fr_vocab_size since the loss looks for 2-dimensional input                \n","                cur_test_loss = criterion(outputs.view(-1, max_fr_words), decoder_output.view(-1))\n","                \n","                cur_test_acc = (pred.view(-1) == decoder_output.view(-1)) \n","                cur_test_acc = cur_test_acc.sum().item()/len(cur_test_acc)\n","                \n","                # loss and acc\n","                test_loss += cur_test_loss\n","                test_acc += cur_test_acc\n","\n","        # epoch output\n","        train_loss = (train_loss/len(train_loader)).item()\n","        train_acc = train_acc/len(train_loader)\n","        val_loss = (test_loss/len(test_loader)).item()\n","        val_acc = test_acc/len(test_loader)\n","        history['train_loss'].append(train_loss)\n","        history['train_acc'].append(train_acc)\n","        history['test_loss'].append(val_loss)\n","        history['test_acc'].append(val_acc)\n","        print(f\"Epoch:{epoch + 1} / {epochs}, train loss:{train_loss:.4f} train_acc:{train_acc:.4f}, valid loss:{val_loss:.4f} valid acc:{val_acc:.4f}\")\n","    \n","    return history"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59FZGTObTDts","executionInfo":{"status":"ok","timestamp":1649264138571,"user_tz":240,"elapsed":646404,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"df8e576e-dce0-4e6a-b5f8-b735ef366402"},"source":["history = train_model(model=model,\n","                      train_dataset = train_dataset,\n","                      test_dataset = test_dataset,\n","                      device=device,\n","                      epochs=50,\n","                      batch_size=64)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Start\n","Epoch:1 / 50, train loss:4.5531 train_acc:0.3292, valid loss:4.0302 valid acc:0.4017\n","Epoch:2 / 50, train loss:3.8761 train_acc:0.4113, valid loss:3.8225 valid acc:0.4244\n","Epoch:3 / 50, train loss:3.6659 train_acc:0.4326, valid loss:3.6454 valid acc:0.4452\n","Epoch:4 / 50, train loss:3.4935 train_acc:0.4547, valid loss:3.5324 valid acc:0.4617\n","Epoch:5 / 50, train loss:3.3514 train_acc:0.4684, valid loss:3.4392 valid acc:0.4745\n","Epoch:6 / 50, train loss:3.2308 train_acc:0.4792, valid loss:3.3732 valid acc:0.4788\n","Epoch:7 / 50, train loss:3.1239 train_acc:0.4876, valid loss:3.3045 valid acc:0.4895\n","Epoch:8 / 50, train loss:3.0282 train_acc:0.4953, valid loss:3.2686 valid acc:0.4915\n","Epoch:9 / 50, train loss:2.9401 train_acc:0.5028, valid loss:3.2083 valid acc:0.4990\n","Epoch:10 / 50, train loss:2.8579 train_acc:0.5095, valid loss:3.1691 valid acc:0.5056\n","Epoch:11 / 50, train loss:2.7828 train_acc:0.5159, valid loss:3.1441 valid acc:0.5090\n","Epoch:12 / 50, train loss:2.7100 train_acc:0.5218, valid loss:3.1253 valid acc:0.5136\n","Epoch:13 / 50, train loss:2.6434 train_acc:0.5280, valid loss:3.0947 valid acc:0.5159\n","Epoch:14 / 50, train loss:2.5794 train_acc:0.5335, valid loss:3.0711 valid acc:0.5185\n","Epoch:15 / 50, train loss:2.5170 train_acc:0.5390, valid loss:3.0439 valid acc:0.5206\n","Epoch:16 / 50, train loss:2.4581 train_acc:0.5447, valid loss:3.0408 valid acc:0.5239\n","Epoch:17 / 50, train loss:2.4029 train_acc:0.5499, valid loss:3.0162 valid acc:0.5249\n","Epoch:18 / 50, train loss:2.3483 train_acc:0.5557, valid loss:2.9895 valid acc:0.5298\n","Epoch:19 / 50, train loss:2.2977 train_acc:0.5602, valid loss:3.0147 valid acc:0.5287\n","Epoch:20 / 50, train loss:2.2465 train_acc:0.5662, valid loss:2.9902 valid acc:0.5307\n","Epoch:21 / 50, train loss:2.1976 train_acc:0.5704, valid loss:2.9681 valid acc:0.5342\n","Epoch:22 / 50, train loss:2.1513 train_acc:0.5753, valid loss:2.9636 valid acc:0.5345\n","Epoch:23 / 50, train loss:2.1056 train_acc:0.5812, valid loss:2.9655 valid acc:0.5345\n","Epoch:24 / 50, train loss:2.0607 train_acc:0.5852, valid loss:2.9499 valid acc:0.5379\n","Epoch:25 / 50, train loss:2.0178 train_acc:0.5903, valid loss:2.9585 valid acc:0.5363\n","Epoch:26 / 50, train loss:1.9750 train_acc:0.5953, valid loss:2.9544 valid acc:0.5360\n","Epoch:27 / 50, train loss:1.9349 train_acc:0.6007, valid loss:2.9348 valid acc:0.5379\n","Epoch:28 / 50, train loss:1.8938 train_acc:0.6061, valid loss:2.9374 valid acc:0.5403\n","Epoch:29 / 50, train loss:1.8537 train_acc:0.6124, valid loss:2.9557 valid acc:0.5379\n","Epoch:30 / 50, train loss:1.8175 train_acc:0.6165, valid loss:2.9421 valid acc:0.5412\n","Epoch:31 / 50, train loss:1.7782 train_acc:0.6219, valid loss:2.9345 valid acc:0.5403\n","Epoch:32 / 50, train loss:1.7408 train_acc:0.6271, valid loss:2.9408 valid acc:0.5376\n","Epoch:33 / 50, train loss:1.7071 train_acc:0.6324, valid loss:2.9594 valid acc:0.5399\n","Epoch:34 / 50, train loss:1.6701 train_acc:0.6394, valid loss:2.9587 valid acc:0.5396\n","Epoch:35 / 50, train loss:1.6373 train_acc:0.6449, valid loss:2.9474 valid acc:0.5399\n","Epoch:36 / 50, train loss:1.6021 train_acc:0.6504, valid loss:2.9681 valid acc:0.5367\n","Epoch:37 / 50, train loss:1.5697 train_acc:0.6561, valid loss:2.9575 valid acc:0.5387\n","Epoch:38 / 50, train loss:1.5372 train_acc:0.6627, valid loss:2.9661 valid acc:0.5383\n","Epoch:39 / 50, train loss:1.5052 train_acc:0.6690, valid loss:2.9667 valid acc:0.5398\n","Epoch:40 / 50, train loss:1.4735 train_acc:0.6762, valid loss:2.9656 valid acc:0.5418\n","Epoch:41 / 50, train loss:1.4434 train_acc:0.6816, valid loss:2.9758 valid acc:0.5395\n","Epoch:42 / 50, train loss:1.4131 train_acc:0.6871, valid loss:2.9868 valid acc:0.5396\n","Epoch:43 / 50, train loss:1.3845 train_acc:0.6939, valid loss:2.9931 valid acc:0.5407\n","Epoch:44 / 50, train loss:1.3536 train_acc:0.7012, valid loss:3.0008 valid acc:0.5384\n","Epoch:45 / 50, train loss:1.3259 train_acc:0.7065, valid loss:3.0062 valid acc:0.5405\n","Epoch:46 / 50, train loss:1.2981 train_acc:0.7131, valid loss:3.0162 valid acc:0.5420\n","Epoch:47 / 50, train loss:1.2707 train_acc:0.7192, valid loss:3.0181 valid acc:0.5380\n","Epoch:48 / 50, train loss:1.2446 train_acc:0.7251, valid loss:3.0168 valid acc:0.5403\n","Epoch:49 / 50, train loss:1.2185 train_acc:0.7307, valid loss:3.0374 valid acc:0.5406\n","Epoch:50 / 50, train loss:1.1916 train_acc:0.7371, valid loss:3.0506 valid acc:0.5402\n"]}]},{"cell_type":"markdown","metadata":{"id":"kKmoE7FpTDtt"},"source":["### Define a model for Inference (i.e. Testing Translation)"]},{"cell_type":"markdown","metadata":{"id":"E7n9m2Z5TDtt"},"source":["3) Decode some sentences to check that the model is working (i.e. turn samples from `data_en` into corresponding samples from `data_fr_out`).\n","\n","Because the training process and `inference process` (decoding sentences) are quite different, we use different models for both, albeit they all leverage the same inner layers where **all the weights have been trained**. \n"]},{"cell_type":"markdown","metadata":{"id":"nejfMQbBTDtt"},"source":["#### Inference without \"teacher forcing\"\n","<img width=\"60%\" alt=\"Seq2seq inference\" src=\"https://blog.keras.io/img/seq2seq/seq2seq-inference.png\" width=\"80%\"> \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1__vDoeLTDtt"},"source":["Now we define a function which returns the translated sentences given an input sentence. \n","- The decoder translates word by word starting with an decoder input `[ <start> ]`\n","- When a word, say $w_t$ is translated, the next decode input at $t+1$ is $[ w_t ]$\n","- It continues until either `<end>` is generated or the `max_fr_len` number of words are generated."]},{"cell_type":"code","metadata":{"id":"8Mpqsli0TDtt","executionInfo":{"status":"ok","timestamp":1649264286733,"user_tz":240,"elapsed":229,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}}},"source":["def decode_sequence(model, input_seq, device):  # input_seq is a English sentence\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","        # convert input_seq to tensor\n","        input_seq = torch.IntTensor(input_seq).to(device)\n","\n","        # Encode the input as state vectors.\n","        _, hidden_state, cell_state = model.Encoder(input_seq)\n","\n","\n","        # Generate empty target sequence of (batch=1, length=1). (i.e. we translate word by word)\n","        target_seq = torch.empty((1,1), dtype=torch.int32, device = device)\n","\n","        # Populate the start symbol of target sequence with the start character.\n","        target_seq = target_seq.fill_(fr_tokenizer.word_index[\"<start>\"])\n","\n","        target_seq = target_seq.to(device)\n","\n","        # Generate word by word using the encode state and the last \n","        # generated word\n","\n","        decoded_sentence = []\n","\n","        while True:\n","\n","            # get decode ouput and hidden states, output shape is [1,1,5000]\n","            output_tokens, h, c = model.Decoder(target_seq, hidden_state, cell_state)\n","\n","\n","            # Get the most likely word\n","            _, sampled_token_index = torch.max(output_tokens, dim = -1)\n","\n","            # flatten the token_index and convert to numpy number\n","            sampled_token_index = sampled_token_index.view(-1).item()\n","\n","            # Look up the word by id\n","            sampled_word = reverse_fr_word_index[sampled_token_index]\n","\n","            # append the word to decoded sentence\n","            decoded_sentence.append(sampled_word)\n","\n","            # Exit condition: either hit max length\n","            # or find stop character.\n","            if (sampled_word == '<end>' or len(decoded_sentence) == max_fr_len):\n","                break\n","\n","            # Update the target sequence with newly generated word.\n","            target_seq = target_seq.fill_(sampled_token_index)\n","\n","            # Update states\n","            hidden_state, cell_state = h, c\n","\n","    return ' '.join(decoded_sentence)"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"na1hHqKtTDtu","colab":{"base_uri":"https://localhost:8080/","height":366},"executionInfo":{"status":"ok","timestamp":1649264179924,"user_tz":240,"elapsed":126,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"606508c5-c9b8-43b0-e969-db2880b92155"},"source":["# Now let's test\n","\n","test = text.iloc[10000:10010]\n","test"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                      en  \\\n","53367           He can't know the truth.   \n","109320  I like hot tea better than cold.   \n","36625              Let's go out tonight?   \n","94947     Do you care to hazard a guess?   \n","95368     Have you gone over the lesson?   \n","51758            Who said I was ashamed?   \n","111860  Tom has to call his grandmother.   \n","3746                       I started it.   \n","66709         Don't be cruel to animals.   \n","44934             What did you just say?   \n","\n","                                            fr  \n","53367      Il ne peut pas connaître la vérité.  \n","109320      Je préfère le thé chaud que froid.  \n","36625                        On sort ce soir ?  \n","94947   Voudriez-vous hasarder une hypothèse ?  \n","95368                   As-tu révisé la leçon?  \n","51758            Qui a dit que j'avais honte ?  \n","111860         Tom doit appeler sa grand-mère.  \n","3746                           Je l'ai initié.  \n","66709    Ne sois pas cruel envers les animaux.  \n","44934                   Que viens-tu de dire ?  "],"text/html":["\n","  <div id=\"df-32f440e0-c7fa-4ef1-b483-106acc50c15d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>en</th>\n","      <th>fr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>53367</th>\n","      <td>He can't know the truth.</td>\n","      <td>Il ne peut pas connaître la vérité.</td>\n","    </tr>\n","    <tr>\n","      <th>109320</th>\n","      <td>I like hot tea better than cold.</td>\n","      <td>Je préfère le thé chaud que froid.</td>\n","    </tr>\n","    <tr>\n","      <th>36625</th>\n","      <td>Let's go out tonight?</td>\n","      <td>On sort ce soir ?</td>\n","    </tr>\n","    <tr>\n","      <th>94947</th>\n","      <td>Do you care to hazard a guess?</td>\n","      <td>Voudriez-vous hasarder une hypothèse ?</td>\n","    </tr>\n","    <tr>\n","      <th>95368</th>\n","      <td>Have you gone over the lesson?</td>\n","      <td>As-tu révisé la leçon?</td>\n","    </tr>\n","    <tr>\n","      <th>51758</th>\n","      <td>Who said I was ashamed?</td>\n","      <td>Qui a dit que j'avais honte ?</td>\n","    </tr>\n","    <tr>\n","      <th>111860</th>\n","      <td>Tom has to call his grandmother.</td>\n","      <td>Tom doit appeler sa grand-mère.</td>\n","    </tr>\n","    <tr>\n","      <th>3746</th>\n","      <td>I started it.</td>\n","      <td>Je l'ai initié.</td>\n","    </tr>\n","    <tr>\n","      <th>66709</th>\n","      <td>Don't be cruel to animals.</td>\n","      <td>Ne sois pas cruel envers les animaux.</td>\n","    </tr>\n","    <tr>\n","      <th>44934</th>\n","      <td>What did you just say?</td>\n","      <td>Que viens-tu de dire ?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32f440e0-c7fa-4ef1-b483-106acc50c15d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-32f440e0-c7fa-4ef1-b483-106acc50c15d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-32f440e0-c7fa-4ef1-b483-106acc50c15d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"IeIZqwfrTDtu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649264183723,"user_tz":240,"elapsed":105,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"a16d72f1-bff8-4af2-968a-83e2ea91456e"},"source":["# preprocess test data\n","data_test = en_tokenizer.texts_to_sequences(test[\"en\"])\n","data_test = keras.preprocessing.sequence.pad_sequences(data_test,\\\n","                                                     maxlen=max_en_len, \\\n","                                                     padding='post')\n","print(data_test[:3])"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 13  36   5   0   0   0   0   0   0   0]\n"," [  2  38 360 600 161 105   0   0   0   0]\n"," [ 45  70   0   0   0   0   0   0   0   0]]\n"]}]},{"cell_type":"code","metadata":{"id":"K3WclY3WTDtu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649264291680,"user_tz":240,"elapsed":285,"user":{"displayName":"Rong Liu","userId":"15250589813520096948"}},"outputId":"73382fea-ddf2-4008-fa9e-3617eca6ad2e"},"source":["for i in range(len(data_test)):\n","    #data_test[i].shape\n","    fr = decode_sequence(model, data_test[i][None,:], device)\n","    print(\"\\nEn: \", test.iloc[i][\"en\"])\n","    print(\"Fr: \", test.iloc[i][\"fr\"])\n","    print(\"Translated: \", fr)"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","En:  He can't know the truth.\n","Fr:  Il ne peut pas connaître la vérité.\n","Translated:  il est la reception de ton pere ? <end>\n","\n","En:  I like hot tea better than cold.\n","Fr:  Je préfère le thé chaud que froid.\n","Translated:  ce qui est plus jeune d habitude d aller en\n","\n","En:  Let's go out tonight?\n","Fr:  On sort ce soir ?\n","Translated:  a qui s il vous plait ? <end>\n","\n","En:  Do you care to hazard a guess?\n","Fr:  Voudriez-vous hasarder une hypothèse ?\n","Translated:  voulez vous un peu de the ? <end>\n","\n","En:  Have you gone over the lesson?\n","Fr:  As-tu révisé la leçon?\n","Translated:  tu as la lumiere le francais ? <end>\n","\n","En:  Who said I was ashamed?\n","Fr:  Qui a dit que j'avais honte ?\n","Translated:  il etait deja fait qui etait trop tard . <end>\n","\n","En:  Tom has to call his grandmother.\n","Fr:  Tom doit appeler sa grand-mère.\n","Translated:  tom a t il a la maison . <end>\n","\n","En:  I started it.\n","Fr:  Je l'ai initié.\n","Translated:  j aime les deux questions et mary . <end>\n","\n","En:  Don't be cruel to animals.\n","Fr:  Ne sois pas cruel envers les animaux.\n","Translated:  comment prenez les choses au travail ? <end>\n","\n","En:  What did you just say?\n","Fr:  Que viens-tu de dire ?\n","Translated:  que tu es en train de temps de boire ?\n"]}]},{"cell_type":"markdown","metadata":{"id":"9waeKFrnTDtv"},"source":["### Seq2Seq model with attention"]},{"cell_type":"markdown","metadata":{"id":"tT_9FizTTDtv"},"source":["Now, let’s talk about attention mechanism. What is it and why do we need it?\n","\n","- Difficult to remember and process long complicated context\n","- Struggle with difference in syntax structures used by languages\n","\n","Implementation of a variety of Seq-2-Seq model can be found here: https://github.com/bentrevett/pytorch-seq2seq\n","\n","There are different ways to implement such an attention mechanism. You can find the Torch implemetation at https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lb3_rzbPTDtw"},"source":["## Take aways"]},{"cell_type":"markdown","metadata":{"id":"4YJAvafWTDtw"},"source":["* Neural Machine Translation contains an encoder and a decoder which both are LSTM layers\n","* Attention mechanism can help align encoder and decoder outputs\n","* You can try the following steps to enhance the translation models:\n","    - Use bidirectional LSTM\n","    - Try other more advanced attention mechanisms\n","    - Also, you may need to work on masking when allocating attention scores."]}]}