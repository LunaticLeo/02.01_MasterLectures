---
title: "FE515_2022A_Lecture_8"
author: "Cheng Lu"
output: pdf_document
---

Learning Objectives:
  1. Bisection Method
  2. Newton-Raphson Method
  3. Gradient Descent
  4. Newton’s Method for Optimization

# 1. Bisection Method

A implementation of the algorithm and the pseudo code

```{r}
# the investigated function
f <- function(x){
  x^2 - 2
}
curve(f, from = -10, to = 10)

# investigated interval
a <- -2
b <- -1
tol <- 0.00001 # tolerance, a small number

# finding root process
while(TRUE){
  c <- (a+b)/2
  if(f(c) == 0 || b - a < tol){
    break
  }
  if(f(a)*f(c) < 0){
    b <- c
  }else{
    a <- c
  }
}

# output the result
cat("Root of f(x) is", c)

# double check the result
f(-1.41421127319336)
```

Wrap up into a function

```{r}
bisection <- function(f, a, b, tol = 0.001){
  while(TRUE){
    c <- (a+b)/2
    if(f(c) == 0 || b - a < tol){
      break
    }
    if(f(a)*f(c) < 0){
      b <- c
    }else{
      a <- c
    }
  }
  print(paste("Root of f(x) is", c))
}

# test the function with the same setting
bisection(f, -2, 1)
```

An improved implementation

```{r}
bisection.new <- function(f, a, b, tol = 0.001, N.max = 100){
  # assign value
  f.a <- f(a)
  f.b <- f(b)
  
  # initial check
  if(f.a*f.b > 0){
    warning("f(a) and f(b) have same sign, output may not be a root")
  }else if(f.a == 0){
    return(a)
  }else if(f.b == 0){
    return(b)
  }else if(is.na(f.a*f.b)){
    return(NA)
  }
  
  # the same searching process
  for(n in 1:N.max){
    c <- (a+b)/2
    f.c <- f(c) # call function 'f' once each iteration
    if(f.c == 0 || abs(b - a) < tol){ # check absolute value
      break
    }
    if(f.a*f.c < 0){
      b <- c
      f.b <- f.c
    }else{
      a <- c
      f.a <- f.c
    }
  }
  
  return(c) # return a value rather than print it out
}
```

Test the new function with multiple case

```{r}
bisection.new(f, -1, -2)

bisection.new(f, -1, 1)
```

(The procedure of implementing an algorithm is summarized in slides P13.)

# 2. Newton-Raphson Method


```{r}
Newton_Raphson <- function(f, df, x0, tol = 0.001, N.max = 100){
  for (n in 1:N.max) {
    x1 <- x0 - f(x0)/df(x0)
    if(abs(x1 - x0) < tol){
      break
    }
    x0 <- x1
  }
  return(x1)
}

f <- function(x) x^2 - 2
df <- function(x) 2*x

Newton_Raphson(f, df, -1)


```

Built-in function

```{r}
uniroot(f, c(0, 2))$root
?uniroot
str(uniroot(f, c(0, 2))) #find root of "f" in [0,2]
```

# 3. Gradient Descent

```{r}
gradient_descent <- function(df, x0, alpha = 0.2, tol = 0.0001, N.max = 100){
  for (n in 1:N.max) {
    x1 <- x0 - alpha*df(x0)
    if(sqrt(sum((x1 - x0)^2)) < tol){
      break
    }
    x0 <- x1
  }
  return(x1)
}


```

Test gradient descent method

```{r}
df <- function(x) 2 * x
gradient_descent(df, -1)
```

Second example

```{r}
# define original function and derived function
f2 <- function(x) x*sin(x)
df2 <- function(x) sin(x) + x*cos(x)

# visualize the function
plot(f2, xlim = c(-20,20), main = "f(x)=x*sin(x)") # draw function

# initial setting for gradient descent
x0 <- 7.5 # initial point
alpha <- 0.2 # learning rate
N.max <- 100 # max number of iteration
tol <- 0.0001 # tolerance
points(x0, f2(x0), col = "blue") # draw initial point with blue color

# search for local optimum
for (n in 1:N.max) {
  x1 <- x0 - alpha*df2(x0)
  points(x1, f2(x1), col = "red") # draw new points with red color
  if(abs(x1 - x0) < tol){
    break
  }
x0 <- x1
}
```

# 4. Newton’s Method for Optimization

```{r}
Newton_optim <- function(df, d2f, x0, alpha = 1, tol = 0.0001, N.max = 100){
  for (n in 1:N.max) {
    x1 <- x0 - alpha*solve(d2f(x0), df(x0))
    if(sqrt(sum((x1 - x0)^2)) < tol){
      break
    }
    x0 <- x1
  }
  return(x1)
}
```

Test

```{r}
df <- function(x) 2*x - 10
d2f <- function(x) 2
Newton_optim(df, d2f, 6)
# f <- function(x) x ^ 3 / 3 - 10 * x
# curve(f, from = -10, to = 10)
```

